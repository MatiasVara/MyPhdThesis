\section{Language Behavioral Interface}
\subsection{Review of Existing Approaches}

The notion of language interface is not well defined in the literature, however this concept is recently the focus of many research on the globalization of modeling languages~\cite{GML_ch1, GML_ch3, GML_ch4, TdegeuleSLE}. It is quite consensual to say that a language interface can have different forms depending on the use we want to make of this interface. Consequently, in this section we discuss the notion of a language interface in the specific context of the expression of coordination patterns. 

To specify coordination patterns at language level, all existing approaches we reviewed rely on a partial knowledge of the syntax and the behavioral semantics of the languages they use. Because the coordination activity require a view of the behavioral semantics of the language we named it \emph{Language Behavioral Interface}. From Section~\ref{ch:background}, we know that, at the model level, the notion of interface can be used to expose information about the model behavior in order to allow its coordination. Similarly, at the language level, a behavioral interface is used to expose a part of the behavioral semantics of languages to allow the expression of coordination patterns. In other words, a language behavioral interface abstracts the behavioral semantics of a language, providing only the information required to coordinate it. 

%\ie a partial representation of concurrency and time-related aspects. In this sense, a language behavioral interface is specific for coordination purpose.    

In most of the state-of-art approaches, authors did not consciously use a language behavioral interface. However we show in this section that they all rely on a dedicated abstraction of the language behavioral semantics that may be contained in an interface. 

\paragraph{Ptolemy/ModHel'X: }
In Ptolemy~\cite{ptoleframebib} and ModHel'X~\cite{modhelxbib}, the behavior of a language is described by a director that encodes a domain in Java, \eg FSM, DE, SDF. To support the communication between different domains, each director implements a same Java interface made of a specific set of methods (\eg \emph{initialize()}, \emph{prefire()}, \emph{fire()}, \emph{postfire()}, \emph{wrapup()}). Such mechanism provides an homogeneous view of all the domain behavioral semantics. While the methods from this interface are implemented differently in all the domains, they must conform a common goal expressed in natural language. For instance, a part of the \emph{fire()} function description is: ``\emph{Typically, the fire() method performs the computation associated with an actor}''. Finally, the coordination in both framework is done according to the hierarchy of actors. It means that the framework is also aware of (a part of) the abstract syntax used by the languages. However, this knowledge is left implicit in the language interface.

The \emph{pro} of this approach is that the framework can technically coordinate actors from any domain without knowledge about the actual implementation of the domain. In this sense, the Java interface plays the role of a language behavioral interface by providing an abstract and homogeneous view of the language behavioral semantics. The methods actually represents the coordination points, \ie elements that can be used to specify the coordination. 

The \emph{cons} of this approach are twofold. First, the Java interface restricts the expression of the semantics to a particular technological domain and it restricts the coordination points to be the call of such methods. It is consequently not possible to add extra coordination points without modifying deeply the framework itself. Second, the ``goal'' of the methods is given by their name and the associated comments in the code. It remains consequently very informal while it is of prime importance that all the domains implement consistently these methods for a correct coordination. Also, both frameworks are based on a unique abstract syntax for all the languages. This can be limitation in some cases but necessary in these cases since the knowledge on the abstract syntax required for the coordination is left implicit.
 	
%\footnote{the Executable interface, cf. http://chess.eecs.berkeley.edu/ptexternal/src/ptII/doc/codeDoc/ptolemy/actor/Executable.html}


\paragraph{Di Natale et al: }
In \cite{dinatale}, they use a translation to a common executable language to provide the semantics of the languages they use (Simulink as functional language and a language to specify execution platforms). The resulting behavioral semantics is then for a part in the translation and for another part in the behavioral semantics of the target language (C in this case). In this approach the coordination is supported by a mapping language also translated in C to make the coordination fulfilled. 
The translational semantics of the mapping language is based on knowledge about the translation semantics of the coordinated languages. For instance, it is aware of the name of some methods, which are generated for each subsystem in the functional language. Such methods are part of the translational semantics of the functional language. 
Such knowledge, used to specify the coordination pattern, is a partial view of the behavioral semantics of the functional language. 

On the behavioral language interface point of view, the \emph{pro} of this approach is very
limited since the interface is left implicit. However, it shows that the notion of behavioral interface also makes sense when the behavioral semantics is given by a translation to another language.

The \emph{cons} then come from the hard coding of the knowledge about the behavioral semantics of one language in the behavioral semantics of the coordination pattern. By doing so, even a minor change in the semantics of the language compromise the coordination. The approach lacks a clear specification of the important methods (their names but also their parameters) that are resulting from the translation of some specific part of a language.

% information about the syntax and semantics of the functional and platform languages is contained into the mapping language. First, it contains correspondences between syntactic elements from both languages, \eg \emph{Task}s from the functional and \emph{Cpu}s from the platform.
 
 
{\color{red}\paragraph{Mascot: }
In \cite{mascotbib}, Matlab models are embedded into SDL models by relying on SDL wrappers. The SDL wrapper has access to a C-wrapper implemented as a set of C functions, which calls the Matlab engine through a set of C-library functions. Some of these functions are \emph{InitProcess()} that initializes a data-flow process in Matlab and \emph{OutStream()} that writes a data-flow variable in Matlab. By relying on these methods, the SDL engine drives the simulation and calls the Matlab engine to process dataflow signals. The coordination is not symmetric since the communication is only done from SDL to Matlab. Consequently, there is no interface for the SDL language. Only Matlab is interfaced by relying on C wrappers that act as an interface made of methods.}\footnote{\todo{\color{red}\textbf{here you were speaking about MODEL interface. I tried to modify but actually you are not speaking at all about LANGUAGE behavioral interface. You have to do it !}}}



\subsection{Requirements}

In Ptolemy and ModHel'X the coordination is based on language behavioral interface. The interface specifies, in intention, the coordination points that any model will provide. This notion of coordination points specified in intention would have been also beneficial for the approach provided by Di Natale et al. to be more flexible with regard to the addition of new language in the framework. It is consequently our first requirement for any language behavioral interface.

%
%
%
%The approaches studied in the previous section get information about the languages that they coordinate. However, only Ptolemy~\cite{ptoleframebib} and Modhel'X~\cite{modhelxbib} made such information explicit by relying on a language interface. Conversely, Di Natale et al.~\cite{dinatale} and MASCOT~\cite{mascotbib} encode such information either into a dedicated language or in wrappers. When such information is hard-coded, the task of adding or changing a language becomes difficult. For instance, in~\cite{dinatale}, the mapping language hides the information about what elements are coordinated. Thus, to add or change a language, a language integrator has to first identify what elements from the languages are used as coordination points. Conversely, in Ptolemy~\cite{ptoleframebib} and ModHel'X~\cite{modhelxbib} such coordination points are explicitly identified into a generic interface. Thus, to add a new language, it is only necessary to provide the correct interface. However, in these approaches, the generic interface only provides a list of methods in which the semantics is given by theirs names. The methods can be interpreted differently depending on the domain, thus remaining very informal. Besides, a language integrator has to understand well the framework to know how to use them. 

The nature of the coordination points itself can vary. At the model level, \cite{garlansoftarchbib} explains that there are important benefits of using events (with implicit invocation) in the component interfaces because it provides strong support for reuse and evolution of components. In event-driven coordination approaches, events act as explicit coordination points and exhibit what can be coordinated. This gives support for control and timed coordination while remaining independent of the internal model implementation, thus ensuring a complete separation between the coordination and the computational concerns. Several causal representations from the concurrency theory are used to capture event-based behavioral interface. A causal representation captures the concurrency, dependency and conflict relationships among actions in a particular program. For instance, an event structure~\cite{eventStructures} is a partial order of events, which specifies the, possibly timed, causality relations as well as conflict relations (\ie exclusion relations) between actions of a concurrent system. This fundamental model is powerful because it totally abstracts data and program structure to focus on the partial ordering of actions. It specifies, \emph{in extension} and \emph{in order}, the set of actions that can be observed during the program execution. An event structure can also be specified \emph{in intention} to represent the set of observable event structures during an execution (see, \eg\cite{ccslbib} or \cite{tagmachinebib}).

In a recent work~\cite{sle13-combemale} elements of event structures are reified at the language level to propose a language behavioral interface based on sets of \emph{event types} and \emph{contraints}. In the following, we summarize this work by focusing on the proposed language behavioral interface. 

The approach proposes the definition of an executable language as a 4-tuple \emph{$<$ AS, DSA, DSE, MoCC $>$} where the \emph{AS} is the Abstract Syntax of the language, the \emph{DSA} defines both the data that represents the execution state of the model and the execution functions that modify this execution state. The \mocc represents the, possibly timed, causalities and synchronization of the system by using some events and constraints between them. Then, the \emph{Domain Specific Events} (\dse) link together the three other parts. A \dse is an event type, defined in the context of a metaclass of the \emph{AS} that links an event from the \mocc with the call to an execution function from the $DSA$. \dse are defined by using a specific language named \ecl (standing for Event Constraint Language~\cite{eclbib}) which is an extension of OCL~\cite{omgocl2bib} with events. \ecl takes benefits from the OCL query language and its possibility to augment an abstract syntax with additional attributes (without any side effects). Consequently, by using \ecl, it is possible to augment \as metaclasses and add \dse. 

Jointly with the \dse, related constraints give a symbolic (intentional) representation of an event structure. With such an interface, the concurrency and time-related aspects of the language behavioral semantics are explicitly exposed.
%
Furthermore, from a \dse, it is possible to get information about the \emph{AS} since they are defined in the context of a metaclass. Then, the context of a \dse can be used to get information from the class where the \dse are defined, for instance the attribute name.   

In this approach, for each model conforming to a language, the model behavioral interface is a specification, in intention, of an event structure whose events (named \mse for Model Specific Event) are instances of the \dse defined in the language interface. While \dse are attached to a metaclass, \mse are linked to one of its instances. The causality and conflict relations of the event structure are a model-specific unfolding of the constraints specified in the language behavioral interface. Just like event structures were initially introduced to unfold the execution of Petri nets, authors use them here to unfold the execution of models. 

In this approach, authors focused on the definition of an executable language in which a partial representation of the language behavioral semantics is exposed by using event types. In that sense, the proposed language behavioral interface is domain specific instead of generic like in Ptolemy~\cite{ptoleframebib} or ModHel'X~\cite{modhelxbib}. 

One of the main interesting aspect of~\cite{sle13-combemale} is that they specify the concurrency and time related aspect of the language behavioral semantics in the language interface.
This aspect seems primordial to be able to reason on how to coordinate different languages.
This our second requirement for a language behavioral interface: it may exhibit (a part of) the concurrency and time-related aspects of the language behavioral semantics. This could have been beneficial for Ptolemy or ModHel'X where the adaptation of the time related aspect is based on a deep knowledge of the internal implementation of the domain. To avoid that, it may be add as an information in the language behavioral interface.

Finally, Ptolemy/ModHel'X through the use of a single abstract syntax and the hierarchical coordination or the framework proposed by Di Natale et al. through the specification of constraints on the mapping model (\eg Task must be mapped on CPU); they all make use of information about the abstract syntax that may be present in the language interface of the language, even if this one is a language behavioral interface.

In this section, we have rationalized two requirements for a language behavioral interface:
\begin{enumerate}
\item it may specify in an intentional way the coordination points that can be used by the coordination on any model conforming the language;
\item it may exhibit (a part of) the concurrency an time related aspect of the behavioral semantics of the language;
\item it may exhibit (a part of) the abstract syntax of the language.
\end{enumerate}

These three requirements seem sufficient to specify a coordination pattern. They provide partial but sufficient information about the behavioral semantics of languages for coordination purpose.  These requirements have all been implemented in a specific way in all the approaches. Also, even if they did not address the coordination between languages~\cite{sle13-combemale} is the only approach to provide this interface consciously.

In the next section, we study how the existing approaches specify the correspondences between languages elements.	

	    	
	    	
	    				%\item In~\cite{taminghetero}, authors rely on the interface automata to specify the 
	    				%\item An interesting research direction could be to adapt the notion of interface automata proposed in~\cite{henzingerIA} to specify the acceptable protocol between the calls of the functions that implements the semantics of the language. 
	    				
	    				
	    				%However, because the language interface definition used in \cite{ptolemybib} and \cite{modhelxbib} provides only a list of functions, it requires a deep understanding of the underlying framework to be able to specify correct coordination patterns. A naming convention on the name of the methods remains very informal since the same methods may be have a different meaning depending on the domain.        
	    				
	    				%\item All the approaches studied retrieve information on the syntax and/or behavioral semantics of the languages they coordinate. Only \cite{ptolemybib} and \cite{modhelxbib} made explicit the notion of interface; \cite{MarcoModels2014} and \cite{mascotbib} retrieved the information implicitly from the definition of the languages. 
	    				%Using an explicit language interface is a way to obtain a language independent representation of the behavioral semantics. In this case, the approaches can add support to new languages with a minimum effort since they are seen homogeneously. However, because the language interface definition used in \cite{ptolemybib} and \cite{modhelxbib} provides only a list of functions, it requires a deep understanding of the underlying framework to be able to specify correct coordination patterns. 
	    				
	    				%\item This task can be helped by .... 
	    				%\item \todo{An interesting research direction could be to adapt the notion of interface automata proposed in~\cite{henzingerIA} to specify the acceptable protocol between the calls of the functions that implements the semantics of the language.} 
	    				