\section{Language Behavioral Interface}
This section presents the notion of \emph{Language Behavioral Interface}. We begin by reviewing the concept of language interface in existing approaches. Then, we present requirements to make the behavioral interface of a language explicit and better customizable.    

\subsection{Review of Existing Approaches}

In the literature, the notion of language interface is not well defined. Recent works have focused on the concept of language interface in the context of the globalization of modeling languages~\cite{GML_ch1,GML_ch3,GML_ch4,degueule2015melange}. From these works, it is consensual that the content of a language interface is purpose-specific, \ie the content varies depending on the use of the interface. In this subsection, we discuss the notion of a language interface for the specific purpose of specifying coordination patterns between languages, \ie a \emph{Language Behavioral Interface}.
	
To specify coordination patterns at language level, reviewed approaches rely on partial knowledge of the syntax and behavioral semantics of the languages they use. At the model level (see Section~\ref{ch:background}), the notion of behavioral interface is used to expose information about the model behavior to allow its coordination. Similarly, at the language level, a behavioral language interface is used to expose only a part of the behavioral semantics of languages to allow the expression of coordination patterns. In other words, a language behavioral interface abstracts the behavioral semantics of a language, providing only the information required to coordinate it. 
 
\todo{To say something about why we talk about SLE2013}

In the following, we show how existing approaches specify coordination patterns by relying on partial information about the syntax and behavioral semantics of the languages they use.  

\paragraph{Ptolemy/ModHel'X: }
In Ptolemy~\cite{ptoleframebib} and ModHel'X~\cite{modhelxbib}, the behavioral semantics of a language is described by a director that encodes a domain in Java, \eg FSM, DE, SDF. To support the communication between different domains, each director implements a generic interface in Java. Such an interface is made of specific set of methods (\eg \emph{initialize()}, \emph{prefire()}, \emph{fire()}, \emph{postfire()}, \emph{wrapup()}). This mechanism provides a homogeneous view of all the domain behavioral semantics. The implementation of these methods depends on the domain. However, they must conform to a common goal, which is expressed in natural language. For instance, a part of the \emph{fire()} function description is: ``\emph{Typically, the fire() method performs the computation associated with an actor}''. 

Both frameworks are based on a unique abstract syntax (\ie actor model) to represent the syntax of any language. Then, they propose composite actors to identify what actor can be refined into another actor. Hence, the coordination is done according to the hierarchy of actors. In addition, only some elements of the language syntax can be refined, \ie represented by a composite actor. For example, in the FSM domain only states can be refined but not the transitions. Thus, frameworks are also aware of (a part of) the abstract syntax of the languages. However, such knowledge is left implicit into the approach.

The \emph{pro} of these approaches is that the framework can technically coordinate actors from any domain without knowledge about the implementation of the actor domain. In this sense, the language behavioral interface correctly provides an abstract and homogeneous view of the language behavioral semantics. The methods represent the coordination points, \ie elements that can be used to specify the coordination. 

The \emph{cons} of this approach are twofold. First, the interface is specified in a particular technological domain, \ie Java. This limits the representation of the language behavioral semantics as a set of methods and the coordination as a set of calls to such methods. It is consequently not possible to add extra coordination points without modifying deeply the framework itself. Second, the meaning of the methods is given by their names together with some comments in the code. This remains very informal since all the domains have to consistently implement these methods for a correct coordination. 

These frameworks base on a fixed syntax in which all languages are represented by using the actor model. This limits the expressiveness of the coordinated language syntax. But also, it makes simple the interface that only contains information about the language behavioral semantics. In addition, the approach left implicit which elements from the syntax of a language can be refined.   

%To sum-sup these approaches, the behavioral semantics of languages are represented by a generic Java interface made of methods. Such an interface only represents the behavioral semantics of languages, but not the syntax because all languages are represented by a fixed syntax. However, the approach left implicit which elements from the syntax of a language can be refined.  
     

% la syntax es fijate, lo q cual hace q no sea necesario una interface con informacion sintactica, sin embargo , solo algunos elementos pueden ser coordination. esa informacion es implicita y no expuesta en el lenguage. Si la sintax de los lenguages no fuera fijada, los elementos sintacticos que pueden ser coordinados deberian estar en la interface.
%This can be limitation in some cases but necessary in these cases since the knowledge on the abstract syntax required for the coordination is left implicit.
 	
%\footnote{the Executable interface, cf. http://chess.eecs.berkeley.edu/ptexternal/src/ptII/doc/codeDoc/ptolemy/actor/Executable.html}


\paragraph{Di Natale et al: }
In \cite{dinatale}, authors use a translation to a common executable language to provide the behavioral semantics of the coordinated languages, \ie a functional language and a platform language, both developed in SysML. In this approach, the coordination is supported by a mapping language, which its behavioral semantics is also translated in C. As a result, the behavioral semantics of both the coordinated languages and the mapping language are represented in the same technology.

In this approach, information about the syntax and semantics of the functional and platform languages is contained into the mapping language. First, it contains correspondences between syntactic elements from both languages, \eg \emph{Task}s from the functional and \emph{Cpu}s from the platform. Second, to express the semantics of such correspondences, the translation semantics of the mapping language is based on knowledge about the translation semantics of the coordinated languages. For instance, it contains the name of the methods that are generated for each subsystem in the functional language. Such methods are part of the translational semantics of the functional language. 

The mapping language is aware of both the syntax and behavioral semantics of the coordinated languages. This information is encoded into the syntax and the behavioral semantics of the mapping language.precisely, the behavioral interface of each coordinated language is left implicit into the mapping language. The major \emph{cons} of this solution is that any change in the semantics of the coordinated languages compromises the semantics of the mapping language. The approach lacks of a clear specification of the important methods (their names but also their parameters) that are resulting from the translation of some specific part of a language. From our point of view, the only \emph{pro} of this approach is that shows that the notion of behavioral interface also makes sense when the behavioral semantics of the languages is given by a translation to another language.

%The \emph{cons} comes from the hard coding of the knowledge about the behavioral semantics of one language in the behavioral semantics of the coordination pattern. 
%By doing so, even a minor change in the semantics of the language compromise the coordination. The approach lacks a clear specification of the important methods (their names but also their parameters) that are resulting from the translation of some specific part of a language.
% information about the syntax and semantics of the functional and platform languages is contained into the mapping language. First, it contains correspondences between syntactic elements from both languages, \eg \emph{Task}s from the functional and \emph{Cpu}s from the platform.
 
 
{\paragraph{Mascot: }
MASCOT is an ad-hoc solution to coordinate SDL and MATLAB. To coordinate these languages, authors have knowledge about the syntax and the behavioral semantics of the languages. For instance, authors know that MATLAB relies on a data-flow MoCC which is based on the notion of streams. Besides, they know that SDL relies on a Finite State Machine which is based on events. Based on this knowledge, authors provide different mechanism to synchronize events from SDL and streams from MATLAB. However, the information about what is coordinated is left implicit into the approach. They rely on partial information about the syntax and behavioral semantics of the languages, but such information is not made explicit by using a language behavioral interface. The pros of this work is that authors have achieved to coordinate two languages that are developed by using different technologies. However, the knowledge about what elements are coordinated is implicit into the approach, thus limiting this approach to these two languages. Thus, the major \emph{cons} is that this work cannot be easily extended to other languages. 
	
%In \cite{mascotbib}, Matlab models are embedded into SDL models by relying on SDL wrappers. The SDL wrapper has access to a C-wrapper implemented as a set of C functions, which calls the Matlab engine through a set of C-library functions. Some of these functions are \emph{InitProcess()} that initializes a data-flow process in Matlab and \emph{OutStream()} that writes a data-flow variable in Matlab. By relying on these methods, the SDL engine drives the simulation and calls the Matlab engine to process dataflow signals. The coordination is not symmetric since the communication is only done from SDL to Matlab. Consequently, there is no interface for the SDL language. Only Matlab is interfaced by relying on C wrappers that act as an interface made of methods.}\footnote{\todo{\color{red}\textbf{here you were speaking about MODEL interface. I tried to modify but actually you are not speaking at all about LANGUAGE behavioral interface. You have to do it !}}}



\paragraph{Combemale et al: }
%descriptionSLE
In a recent work~\cite{sle13-combemale} the authors propose to specify an executable language as a 4-tuple \emph{$<$ AS, DSA, DSE, MoCC $>$} where the \emph{AS} is the Abstract Syntax of the language, the \emph{DSA} defines both the data that represents the execution state of the model and the execution functions that modify this execution state. The \mocc represents the, possibly timed, causalities and synchronization of the system by using some events and constraints between them. Then, the \emph{Domain Specific Events} (\dse) link together the three other parts. A \dse is an event type, defined in the context of a metaclass of the \emph{AS} that links an event from the \mocc with the call to an execution function from the $DSA$. \dse are defined by using a specific language named \ecl (standing for Event Constraint Language~\cite{eclbib}) which is an extension of OCL~\cite{omgocl2bib} with events. \ecl takes benefits from the OCL query language and its possibility to augment an abstract syntax with additional attributes (without any side effects). Consequently, by using \ecl, it is possible to augment \as metaclasses and add \dse. 

This approach reifies elements of the behavioral semantics of the language to propose an explicit language behavioral interface. The interface is based on sets of \dse (\emph{event types}) and \emph{contraints}. Jointly with the \dse, the related constraints give a symbolic (intentional) representation of an event structure. With such an interface, the concurrency and time-related aspects of the language behavioral semantics are explicitly exposed.
%
Furthermore, from a \dse, it is possible to get information about the \emph{AS} since they are defined in the context of a metaclass. Then, the context of a \dse can be used to get information from the AS.   
For each model conforming to a language, the model behavioral interface is a specification, in intention, of an event structure whose events (named \mse for Model Specific Event) are instances of the \dse defined in the language interface. While \dse are attached to a metaclass, \mse are linked to one of its instances. The causality and conflict relations of the event structure are a model-specific unfolding of the constraints specified in the language behavioral interface. Just like event structures were initially introduced to unfold the execution of Petri nets, authors use them here to unfold the execution of models. The \dse are then a specification of the coordination points that the model will propose.

Note that the partial representation of the language behavioral semantics is exposed by using \dse (Domain Specific Event types). In that sense, the proposed language behavioral interface is domain specific instead of generic like in Ptolemy~\cite{ptoleframebib} or ModHel'X~\cite{modhelxbib}. 

The \emph{pro} of this approach is to make explicit a language behavioral interface, which provides an intentional specification of the coordination points, together with information about the concurrent and time-related aspects of the language behavioral semantics. In addition, the language behavioral interface exposes a part of the abstract syntax.   

The \emph{cons} of this approach comes from the lack of techniques and methods to take advantage of their interface. For instance, it would be interesting to understand what kind of analysis can be driven based on their interface.

%fin description SlE


\subsection{Requirements}
Based on the reviewed approaches, we want to highlight that all approaches rely on partial information about the language they use. However, they did not do so consciously. In other words, approaches did not make such information explicit in a language behavioral interface. In the following, we present some requirements to improve the way that approaches represent such information about the language they use. These requirements aim at providing a language behavioral interface explicit and better customizable.     

In Ptolemy and ModHel'X, the coordination is based on a generic language behavioral interface. The interface specifies, in intention, the coordination points that any model provide. Combemale et al. also specify coordination points in intention by using \dse. This notion of coordination points specified in intention would have been also beneficial for the approach provided by Di Natale et al. to be more flexible with regard to the addition of a new language. Thus, our first requirement is that a language behavioral interface should specified in intention the coordination points. More precisely, it should expose what elements from the languages can be coordinated.

The nature of the coordination points varies from one approach to other. In Ptolemy and ModHel'X, they are implemented by methods, whereas in Combemale et al., they are implemented by using event types. At the model level, \cite{garlansoftarchbib} explains that there are important benefits of using events (with implicit invocation) in the component interfaces because it provides strong support for reuse and evolution of components. This gives support for control and timed coordination while remaining independent of the internal model implementation, thus ensuring a complete separation between the coordination and the computational concerns. Several causal representations from the concurrency theory are used to capture event-based behavioral interface. A causal representation captures the concurrency, dependency and conflict relationships among actions in a particular program. For instance, an event structure~\cite{eventStructures} is a partial order of events, which specifies the, possibly timed, causality relations as well as conflict relations (\ie exclusion relations) between actions of a concurrent system. This fundamental model is powerful because it totally abstracts data and program structure to focus on the partial ordering of actions. It specifies, \emph{in extension} and \emph{in order}, the set of actions that can be observed during the program execution. An event structure can also be specified \emph{in intention} to represent the set of observable event structures during an execution (see, \eg\cite{ccslbib} or \cite{tagmachinebib}). 

This kind of mechanisms can be interesting to provide an abstraction of a language behavioral semantics by exposing its concurrent and time related aspects. This seems primordial to allow reasoning about how to coordinate different languages. Thus, our second requirement is that a language behavioral interface should exhibit (a part of) the concurrency and time-related aspects of the language behavioral semantics. This could have been beneficial for Ptolemy or ModHel'X where the adaptation of the time related aspect is based on a deep knowledge of the internal implementation of the domains. Instead, by adding only the necessary information in the interface, such an adaptation may be ease without all the knowledge about implementation. 

Together with information about the language behavioral semantics, all the reviewed approaches also make use of information about the syntax of the coordinated languages. For instance in Di Natale et al., the mapping language contains information about the syntax of the functional (\eg \emph{Tasks}) and the platform languages (\eg \emph{CPU}). Our third requirement is that the language behavioral interface should also contain information about the abstract syntax of the coordinated languages.     

In this subsection, we have identified three requirements for a language behavioral interface:
\begin{enumerate}
\item It should specify in intention the coordination points that can be used to coordinate any  any model conforming to the language;
\item It should exhibit (a part of) the concurrency an time related aspect of the behavioral semantics of the language;
\item It should exhibit (a part of) the abstract syntax of the language.
\end{enumerate}

In the light of the existing approaches, these three requirements seem necessary to specify a coordination pattern between languages. They provide partial but sufficient information about the behavioral semantics of languages for coordination purpose. Each of the reviewed approaches fulfills at least one of these requirements. Also, even if they did not address the coordination between languages~\cite{sle13-combemale} is the only approach to provide this interface consciously. In the next section, we study how the existing approaches specify the correspondences between languages elements.	

	    	
	    	
	    				%\item In~\cite{taminghetero}, authors rely on the interface automata to specify the 
	    				%\item An interesting research direction could be to adapt the notion of interface automata proposed in~\cite{henzingerIA} to specify the acceptable protocol between the calls of the functions that implements the semantics of the language. 
	    				
	    				
	    				%However, because the language interface definition used in \cite{ptolemybib} and \cite{modhelxbib} provides only a list of functions, it requires a deep understanding of the underlying framework to be able to specify correct coordination patterns. A naming convention on the name of the methods remains very informal since the same methods may be have a different meaning depending on the domain.        
	    				
	    				%\item All the approaches studied retrieve information on the syntax and/or behavioral semantics of the languages they coordinate. Only \cite{ptolemybib} and \cite{modhelxbib} made explicit the notion of interface; \cite{MarcoModels2014} and \cite{mascotbib} retrieved the information implicitly from the definition of the languages. 
	    				%Using an explicit language interface is a way to obtain a language independent representation of the behavioral semantics. In this case, the approaches can add support to new languages with a minimum effort since they are seen homogeneously. However, because the language interface definition used in \cite{ptolemybib} and \cite{modhelxbib} provides only a list of functions, it requires a deep understanding of the underlying framework to be able to specify correct coordination patterns. 
	    				
	    				%\item This task can be helped by .... 
	    				%\item \todo{An interesting research direction could be to adapt the notion of interface automata proposed in~\cite{henzingerIA} to specify the acceptable protocol between the calls of the functions that implements the semantics of the language.} 
	    				