\section{Language Behavioral Interface}
\subsection{Overview}
To specify the coordination at language level, approaches rely on partial knowledge on the syntax and behavioral semantic of the languages they coordinate, \ie a language behavioral interface. At the model level, the notion of interface is used to expose information of model behavior in order to ease its coordination (see Section~\ref{ch:background}). Similarly, at language level, a behavioral interface is used to expose only a part of the syntax and behavioral semantics of languages to ease its coordination. It abstracts the behavioral semantics of a language, thus providing only the information required to coordinate it, \ie a partial representation of concurrency and time-related aspects. In this sense, a language behavioral interface is specific for coordination purpose.    

In the state-of-art approaches, the notion of language behavioral varies depending on the approach. For example in Ptolemy~\cite{ptoleframebib} and ModHel'X~\cite{modhelxbib}, the behavior of models is described by a director that implements a domain in Java, \eg FSM, DE, SDF. To support the communication between different domains, each director has a Java interface made of methods that provides a homogeneous view of the behavioral semantics of the domain. Each method has a semantics that is expressed in natural language. For instance, a part of the \emph{fire()} function description is: ``\emph{Typically, the fire() method performs the computation associated with an actor}''.
 	
%\footnote{the Executable interface, cf. http://chess.eecs.berkeley.edu/ptexternal/src/ptII/doc/codeDoc/ptolemy/actor/Executable.html}
While in frameworks the notion of interface is clearly defined, in Di Natale et al.~\cite{dinatale} information about the syntax and semantics of the functional and platform languages is contained into the mapping language. First, it contains correspondences between syntactic elements from both languages, \eg \emph{Task}s from the functional and \emph{Cpu}s from the platform. Second, to express the semantics of such correspondences, the translation semantics of the mapping language is based on knowledge about the translation semantics of the coordinated languages. For instance, it contains the name of the methods that are generated for each subsystem in the functional language. Such methods are part of the translational semantics of the functional language. The mapping language encodes the knowledge about what elements from the languages are coordinated. In this approach, such a notion of interface is implicit into the mapping language. 

In Mascot~\cite{mascotbib}, Matlab models are embedded into SDL models by relying on SDL wrappers. The SDL wrapper has access to a C-wrapper implemented as a set of C functions, which calls the Matlab engine through a set of C-library functions. Some of these functions are \emph{InitProcess()} that initializes a data-flow process in Matlab and \emph{OutStream()} that writes a data-flow variable in Matlab. By relying on these methods, the SDL engine drives the simulation and calls the Matlab engine to process dataflow signals. The coordination results not symmetric since the communication is only done from SDL to Matlab. In this sense, there is non an interface for SDL models. Only Matlab is interfaced by relying on C wrappers that act as an interface made of methods.
\subsection{Discussion}
The approaches studied in the previous section get information about the languages that they coordinate. However, only Ptolemy~\cite{ptoleframebib} and Modhel'X~\cite{modhelxbib} made such information explicit by relying on a language interface. Conversely, Di Natale et al.~\cite{dinatale} and MASCOT~\cite{mascotbib} encode such information either into a dedicated language or in wrappers. When such information is hard-coded, the task of adding or changing a language becomes difficult. For instance, in~\cite{dinatale}, the mapping language hides the information about what elements are coordinated. Thus, to add or change a language, a language integrator has to first identify what elements from the languages are used as coordination points. Conversely, in Ptolemy~\cite{ptoleframebib} and ModHel'X~\cite{modhelxbib} such coordination points are explicitly identified into a generic interface. Thus, to add a new language, it is only necessary to provide the correct interface. However, in these approaches, the generic interface only provides a list of methods in which the semantics is given by theirs names. The methods can be interpreted differently depending on the domain, thus remaining very informal. Besides, a language integrator has to understand well the framework to know how to use them. 

At the model level,~\cite{garlansoftarchbib} explains that there are important benefits of using events (with implicit invocation) in the component interfaces because it provides strong support for reuse and evolution of components. In event-driven coordination approaches, events act as explicit coordination points and exhibit what can be coordinated. This gives support for control and timed coordination while remaining independent of the internal model implementation, thus ensuring a complete separation between the coordination and the computational concerns. Several causal representations from the concurrency theory are used to capture event-based behavioral interface. A causal representation captures the concurrency, dependency and conflict relationships among actions in a particular program. For instance, an event structure~\cite{eventStructures} is a partial order of events, which specifies the, possibly timed, causality relations as well as conflict relations (\ie exclusion relations) between actions of a concurrent system. This fundamental model is powerful because it totally abstracts data and program structure to focus on the partial ordering of actions. It specifies, \emph{in extension} and \emph{in order}, the set of actions that can be observed during the program execution. An event structure can also be specified \emph{in intention} to represent the set of observable event structures during an execution (see, \eg\cite{ccslbib} or \cite{tagmachinebib}).

In a recent work~\cite{sle13-combemale} elements of event structures are reified at the language level to propose a language behavioral interface based on sets of \emph{event types} and \emph{contraints}. In the following, we summarize this work by focusing on the proposed language behavioral interface. 

The approach proposes the definition of an executable language as a 4-tuple \emph{$<$ AS, DSA, DSE, MoCC $>$} where the \emph{AS} is the Abstract Syntax of the language, the \emph{DSA} defines both the data that represents the execution state of the model and the execution functions that modify this execution state. The \mocc represents the, possibly timed, causalities and synchronization of the system by using some events and constraints between them. Then, the \emph{Domain Specific Events} (\dse) link together the three other parts. A \dse is an event type, defined in the context of a metaclass of the \emph{AS} that links an event from the \mocc with the call to an execution function from the $DSA$. \dse are defined by using a specific language named \ecl (standing for Event Constraint Language~\cite{eclbib}) which is an extension of OCL~\cite{omgocl2bib} with events. \ecl takes benefits from the OCL query language and its possibility to augment an abstract syntax with additional attributes (without any side effects). Consequently, by using \ecl, it is possible to augment \as metaclasses and add \dse. 

Jointly with the \dse, related constraints give a symbolic (intentional) representation of an event structure. With such an interface, the concurrency and time-related aspects of the language behavioral semantics are explicitly exposed. Furthermore, from a \dse, it is possible to get information about the \emph{AS} since they are defined in the context of a metaclass. Then, the context of a \dse can be used to get information from the class where the \dse are defined, for instance the attribute name.   

In this approach, for each model conforming to a language, the model behavioral interface is a specification, in intention, of an event structure whose events (named \mse for Model Specific Event) are instances of the \dse defined in the language interface. While \dse are attached to a metaclass, \mse are linked to one of its instances. The causality and conflict relations of the event structure are a model-specific unfolding of the constraints specified in the language behavioral interface. Just like event structures were initially introduced to unfold the execution of Petri nets, authors use them here to unfold the execution of models. 

In this approach, authors focused on the definition of an executable language in which a partial representation of the language behavioral semantics is exposed by using event types. In that sense, the proposed language behavioral interface is domain specific instead of generic like in Ptolemy~\cite{ptoleframebib} or ModHel'X~\cite{modhelxbib}. 

In this section, we have presented the notion of language behavioral interface which contains partial information about the syntax and behavioral semantics of languages for coordination purpose. In particular in~\cite{sle13-combemale}, authors proposed a language behavioral interface which is domain-specific and made of event types. Authors did not address the coordination between languages, but, they gave the first steps towards the reification of the coordination at language level by relying on the elements of the language behavioral interface. In the next section, we study how approaches specify correspondences between languages elements.	

	    	
	    	
	    				%\item In~\cite{taminghetero}, authors rely on the interface automata to specify the 
	    				%\item An interesting research direction could be to adapt the notion of interface automata proposed in~\cite{henzingerIA} to specify the acceptable protocol between the calls of the functions that implements the semantics of the language. 
	    				
	    				
	    				%However, because the language interface definition used in \cite{ptolemybib} and \cite{modhelxbib} provides only a list of functions, it requires a deep understanding of the underlying framework to be able to specify correct coordination patterns. A naming convention on the name of the methods remains very informal since the same methods may be have a different meaning depending on the domain.        
	    				
	    				%\item All the approaches studied retrieve information on the syntax and/or behavioral semantics of the languages they coordinate. Only \cite{ptolemybib} and \cite{modhelxbib} made explicit the notion of interface; \cite{MarcoModels2014} and \cite{mascotbib} retrieved the information implicitly from the definition of the languages. 
	    				%Using an explicit language interface is a way to obtain a language independent representation of the behavioral semantics. In this case, the approaches can add support to new languages with a minimum effort since they are seen homogeneously. However, because the language interface definition used in \cite{ptolemybib} and \cite{modhelxbib} provides only a list of functions, it requires a deep understanding of the underlying framework to be able to specify correct coordination patterns. 
	    				
	    				%\item This task can be helped by .... 
	    				%\item \todo{An interesting research direction could be to adapt the notion of interface automata proposed in~\cite{henzingerIA} to specify the acceptable protocol between the calls of the functions that implements the semantics of the language.} 
	    				