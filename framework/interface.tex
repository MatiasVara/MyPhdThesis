\section{Language Behavioral Interface}
This section presents the notion of \emph{Language Behavioral Interface}. We begin by reviewing the concept of language interface in existing approaches, and then, we present requirements to make the behavioral interface of a language explicit and better customizable.    

\subsection{Review of Existing Approaches}

The notion of language interface has been recently the focus of some works in the context of the globalization of modeling languages~\cite{GML_ch1,GML_ch3,GML_ch4,degueule2015melange}. From these works, it is consensual that the content of a language interface is purpose-specific, \ie the content varies depending on the use of the interface. In particular in this subsection, we discuss the notion of a language interface for the specific purpose of specifying coordination patterns between languages. Since the coordination activity requires a view of the behavioral semantics of languages, we name such an interface \emph{Language Behavioral Interface}.
	
From Chapter~\ref{ch:background}, at model level the notion of behavioral interface is used to expose information about the model behavior to allow its coordination. Similarly, at the language level, a language behavioral interface is used to expose only a part of the behavioral semantics of languages to allow the expression of coordination patterns. In other words, a language behavioral interface abstracts the behavioral semantics of a language, providing only the information required to coordinate it. 
 
In the following, we show how existing approaches specify coordination patterns by relying on partial information about the syntax and behavioral semantics of the languages they use. In addition, we review the work done in~\cite{sle13-combemale} in which authors propose to specify an executable language that exposes a part of its behavioral semantics. This work will help us to understand the benefits of an explicit language behavioral interface.    

\paragraph{Ptolemy~\cite{ptoleframebib}/ModHel'X~\cite{modhelxbib}:}
In these approaches, the behavioral semantics of a language is described by a director that encodes a domain in Java, \eg FSM, DE, SDF. To support the communication between different domains, each director implements a generic interface in Java. Such an interface is made of specific set of methods (\eg \emph{initialize()}, \emph{prefire()}, \emph{fire()}, \emph{postfire()}, \emph{wrapup()}). This mechanism provides a homogeneous view of all the domain behavioral semantics. The implementation of these methods depends on the domain. However, they must conform to a common goal, which is expressed in natural language. For instance, a part of the \emph{fire()} function description is: ``\emph{Typically, the fire() method performs the computation associated with an actor}''. 

Both frameworks are based on a unique abstract syntax (\ie actor model) to represent the syntax of any language. Then, they propose composite actors to identify what actor can be refined into another actor. Hence, the coordination is done according to the hierarchy of actors. In addition, only some elements of the language syntax can be represented by a composite actor. For example, in the FSM domain only states can be a composited actor, but not the transitions. This makes that only some elements of the language syntax can contain other actors. Thus, frameworks are also aware of (a part of) the abstract syntax of the coordinated language. However, such knowledge is left implicit into the framework implementation.

The \emph{pro} of these approaches is that the framework can technically coordinate actors from any domain without knowledge about the implementation of the actor domain. In this sense, the language behavioral interface correctly provides an abstract and homogeneous view of the language behavioral semantics. The methods represent the coordination points, \ie elements that can be used to specify the coordination between different domains. 

The \emph{cons} of these approaches are twofold. First, the interface is specified in a particular technological domain, \ie Java. This limits the representation of the language behavioral semantics as a set of methods and the coordination as a set of calls to such methods. It is consequently not possible to add extra coordination points without modifying deeply the framework itself. Second, all the domains have to implement consistently these methods, however, the meaning of the methods is given by their names together with some comments in the code. This remains very informal to ensure a correct implementation of these methods.  

These frameworks base on a fixed syntax in which all languages are represented by using the actor model. This limits the expressiveness of the coordinated language syntax. However, it makes simple the interface that only contains information about the language behavioral semantics. In addition, the approach left implicit which elements from the syntax of a language can be refined, \ie can be a composite actor.   

%To sum-sup these approaches, the behavioral semantics of languages are represented by a generic Java interface made of methods. Such an interface only represents the behavioral semantics of languages, but not the syntax because all languages are represented by a fixed syntax. However, the approach left implicit which elements from the syntax of a language can be refined.  
     

% la syntax es fijate, lo q cual hace q no sea necesario una interface con informacion sintactica, sin embargo , solo algunos elementos pueden ser coordination. esa informacion es implicita y no expuesta en el lenguage. Si la sintax de los lenguages no fuera fijada, los elementos sintacticos que pueden ser coordinados deberian estar en la interface.
%This can be limitation in some cases but necessary in these cases since the knowledge on the abstract syntax required for the coordination is left implicit.
 	
%\footnote{the Executable interface, cf. http://chess.eecs.berkeley.edu/ptexternal/src/ptII/doc/codeDoc/ptolemy/actor/Executable.html}


\paragraph{Di Natale et al.~\cite{dinatale}:}
In this work, authors use a translation to a common executable language (\ie C++) to provide the behavioral semantics of the coordinated languages. The coordination is supported by a mapping language, which its behavioral semantics is also translated in C++. As a result, the behavioral semantics of both the coordinated languages and the mapping language are represented in the same technology.

In this approach, information about the syntax and semantics of the functional and platform languages is contained into the mapping language. First, it contains correspondences between syntactic elements from both languages, \eg \emph{Task}s from the functional and \emph{Cpu}s from the platform. Second, to express the semantics of such correspondences, the translation semantics of the mapping language is based on knowledge about the translation semantics of the coordinated languages. For instance, it contains the name of the methods that are generated for each subsystem in the functional language. Such methods are part of the translational semantics of the functional language. 

The mapping language is aware of both the syntax and behavioral semantics of the coordinated languages. This information is encoded into the syntax and the behavioral semantics of the mapping language. More precisely, the language behavioral interface of each coordinated language is left implicit into the mapping language. The major \emph{cons} of this solution is that any change in the semantics of the coordinated languages compromises the semantics of the mapping language. The approach lacks of a clear specification of the coordination points, \ie the important methods (their names but also their parameters) that are resulting from the translation of some specific part of a language. 

The only \emph{pro} of this approach is that shows that the notion of language behavioral interface also makes sense when the behavioral semantics of the languages is given by a translation to another language.

%The \emph{cons} comes from the hard coding of the knowledge about the behavioral semantics of one language in the behavioral semantics of the coordination pattern. 
%By doing so, even a minor change in the semantics of the language compromise the coordination. The approach lacks a clear specification of the important methods (their names but also their parameters) that are resulting from the translation of some specific part of a language.
% information about the syntax and semantics of the functional and platform languages is contained into the mapping language. First, it contains correspondences between syntactic elements from both languages, \eg \emph{Task}s from the functional and \emph{Cpu}s from the platform.
 
 
{\paragraph{Mascot~\cite{mascotbib}:}
This is an ad-hoc solution to coordinate SDL and MATLAB. To coordinate these languages, authors have knowledge about the syntax and the behavioral semantics of the languages. For instance, authors know that MATLAB relies on a data-flow MoCC which is based on the notion of streams. Besides, they know that SDL relies on a Finite State Machine which is based on events. Based on this knowledge, authors provide different mechanism to synchronize events from SDL and streams from MATLAB. However, the information about what is coordinated is left implicit into the approach. They rely on partial information about the syntax and behavioral semantics of the languages, but such information is not made explicit by using a language behavioral interface. The \emph{pros} of this work is that authors have achieved to coordinate two languages that are developed by using different technologies. However, the knowledge about what elements are coordinated is implicit into the approach, thus limiting this approach to these two languages. Thus, the major \emph{cons} is that this work cannot be easily extended to other languages. 
	
%In \cite{mascotbib}, Matlab models are embedded into SDL models by relying on SDL wrappers. The SDL wrapper has access to a C-wrapper implemented as a set of C functions, which calls the Matlab engine through a set of C-library functions. Some of these functions are \emph{InitProcess()} that initializes a data-flow process in Matlab and \emph{OutStream()} that writes a data-flow variable in Matlab. By relying on these methods, the SDL engine drives the simulation and calls the Matlab engine to process dataflow signals. The coordination is not symmetric since the communication is only done from SDL to Matlab. Consequently, there is no interface for the SDL language. Only Matlab is interfaced by relying on C wrappers that act as an interface made of methods.}\footnote{\todo{\color{red}\textbf{here you were speaking about MODEL interface. I tried to modify but actually you are not speaking at all about LANGUAGE behavioral interface. You have to do it !}}}



\paragraph{Combemale et al.~\cite{sle13-combemale}:}
%descriptionSLE
In a recent work~\cite{sle13-combemale} the authors propose to specify an executable language as a 4-tuple \emph{$<$ AS, DSA, DSE, MoCC $>$} where the \emph{AS} is the Abstract Syntax of the language, the \emph{DSA} defines both the data that represents the execution state of the model and the execution functions that modify this execution state. The \mocc represents the, possibly timed, causalities and synchronization of the system by using some events and constraints between them. Then, the \emph{Domain Specific Events} (\dse) link together the three other parts. A \dse is an event type, defined in the context of a metaclass of the \emph{AS} that links an event from the \mocc with the call to an execution function from the $DSA$. \dse are defined by using a specific language named \ecl (standing for Event Constraint Language~\cite{eclbib}) which is an extension of OCL~\cite{omgocl2bib} with events. \ecl takes benefits from the OCL query language and its possibility to augment an abstract syntax with additional attributes (without any side effects). Consequently, by using \ecl, it is possible to augment \as metaclasses and add \dse. 

This approach reifies elements of the behavioral semantics of the language to propose an explicit language behavioral interface. The interface is based on sets of \dse (\emph{event types}) and \emph{contraints}. Jointly with the \dse, the related constraints give a symbolic (intentional) representation of an event structure. With such an interface, the concurrency and time-related aspects of the language behavioral semantics are explicitly exposed.
%
Furthermore, from a \dse, it is possible to get information about the \emph{AS} since they are defined in the context of a metaclass. Then, the context of a \dse can be used to get information from the AS.   
For each model conforming to a language, the model behavioral interface is a specification, in intention, of an event structure whose events (named \mse for Model Specific Event) are instances of the \dse defined in the language interface. While \dse are attached to a metaclass, \mse are linked to one of its instances. The causality and conflict relations of the event structure are a model-specific unfolding of the constraints specified in the language behavioral interface. Just like event structures were initially introduced to unfold the execution of Petri nets, authors use them here to unfold the execution of models. The \dse are then a specification of the coordination points that the model will propose.

Note that the partial representation of the language behavioral semantics is exposed by using \dse (Domain Specific Event types). In that sense, the proposed language behavioral interface is domain specific instead of generic like in Ptolemy~\cite{ptoleframebib} or ModHel'X~\cite{modhelxbib}. 

The \emph{pro} of this approach is to make explicit a language behavioral interface, which provides an intentional specification of the coordination points, together with information about the concurrent and time-related aspects of the language behavioral semantics. In addition, the language behavioral interface exposes a part of the abstract syntax.   

In this work, however, authors do not propose any technique or method to take advantages of their interface. For instance, it would be interesting to understand what kind of analysis can be driven based on their interface.

%fin description SlE

\subsection{Requirements}
We want to highlight that all approaches rely on partial information about the languages they use. However, they have not achieved to systematically express such information in a language behavioral interface. To improve the way that approaches represent information about the coordinated languages, we propose the following requirements: 

\begin{enumerate}
	\item A language behavioral interface should specify in intention the coordination points;
	\item A language behavioral interface should expose (a part of) the concurrency and the time-related aspects of the language behavioral semantics;
	\item A language behavioral interface should expose (a part of) the abstract syntax.
\end{enumerate}
    
Let us consider each of these requirements in turn. In Ptolemy and ModHel'X, the coordination is based on a language behavioral interface that is generic. The interface specifies, in intention, the coordination points (\ie Java methods) that any model provides. Similarly, Combemale et al. specify coordination points in intention by using \dse. This notion of coordination points specified in intention would have been also beneficial for the approach provided by Di Natale et al. to be more flexible with regard to the addition of a new language.

The nature of the coordination points varies from one approach to other. In Ptolemy and ModHel'X, they are implemented by methods, whereas in Combemale et al., they are implemented by using event types. At the model level, \cite{garlansoftarchbib} explains that there are important benefits of using events (with implicit invocation) in the component interfaces because it provides strong support for reuse and evolution of components. This gives support for control and timed coordination while remaining independent of the internal model implementation, thus ensuring a complete separation between the coordination and the computational concerns. Several causal representations from the concurrency theory are used to capture event-based behavioral interface. A causal representation captures the concurrency, dependency and conflict relationships among actions in a particular program. For instance, an event structure~\cite{eventStructures} is a partial order of events, which specifies the, possibly timed, causality relations as well as conflict relations (\ie exclusion relations) between actions of a concurrent system. This fundamental model is powerful because it totally abstracts data and program structure to focus on the partial ordering of actions. It specifies, \emph{in extension} and \emph{in order}, the set of actions that can be observed during the program execution. An event structure can also be specified \emph{in intention} to represent the set of observable event structures during an execution (see, \eg\cite{ccslbib} or \cite{tagmachinebib}). 

These mechanisms provide an abstraction of a language behavioral semantics by exposing concurrent and time related aspects. This information allows reasoning about how to coordinate different languages. This could have been beneficial for Ptolemy or ModHel'X where the adaptation of the time related aspect is based on a deep knowledge of the internal implementation of the domains. Instead, by adding only the necessary information in the interface, such an adaptation could be easier without all the knowledge about implementation. 

Together with information about the language behavioral semantics, all the reviewed approaches also make use of information about the syntax of the coordinated languages. For instance in Di Natale et al., the mapping language contains information about the syntax of the functional (\eg \emph{Tasks}) and the platform languages (\eg \emph{CPU}). Ptolemy and Modhel'X also contains information about which elements from the language syntax can be defined as composite actors.

%In this subsection, we have identified three requirements for a language behavioral interface:
%\begin{enumerate}
%\item It should specify in intention the coordination points that can be used to coordinate any model conforming to the language;
%\item It should exhibit (a part of) the concurrency an time related aspect of the behavioral semantics of the language;
%\item It should exhibit (a part of) the abstract syntax of the language.
%\end{enumerate}

In this section, we presented three requirements for a language behavioral interface to specify a coordination pattern between languages. These requirements aim at providing the needed information about the behavioral semantics of languages for coordination purpose. Although~\cite{sle13-combemale} did not address the coordination between languages, it is the only approach to provide this interface consciously. In the next section, we study how existing approaches get correspondences between model elements by relying on correspondence rules.

	    	
	    	
	    				%\item In~\cite{taminghetero}, authors rely on the interface automata to specify the 
	    				%\item An interesting research direction could be to adapt the notion of interface automata proposed in~\cite{henzingerIA} to specify the acceptable protocol between the calls of the functions that implements the semantics of the language. 
	    				
	    				
	    				%However, because the language interface definition used in \cite{ptolemybib} and \cite{modhelxbib} provides only a list of functions, it requires a deep understanding of the underlying framework to be able to specify correct coordination patterns. A naming convention on the name of the methods remains very informal since the same methods may be have a different meaning depending on the domain.        
	    				
	    				%\item All the approaches studied retrieve information on the syntax and/or behavioral semantics of the languages they coordinate. Only \cite{ptolemybib} and \cite{modhelxbib} made explicit the notion of interface; \cite{MarcoModels2014} and \cite{mascotbib} retrieved the information implicitly from the definition of the languages. 
	    				%Using an explicit language interface is a way to obtain a language independent representation of the behavioral semantics. In this case, the approaches can add support to new languages with a minimum effort since they are seen homogeneously. However, because the language interface definition used in \cite{ptolemybib} and \cite{modhelxbib} provides only a list of functions, it requires a deep understanding of the underlying framework to be able to specify correct coordination patterns. 
	    				
	    				%\item This task can be helped by .... 
	    				%\item \todo{An interesting research direction could be to adapt the notion of interface automata proposed in~\cite{henzingerIA} to specify the acceptable protocol between the calls of the functions that implements the semantics of the language.} 
	    				