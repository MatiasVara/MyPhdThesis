\section{Language Behavioral Interface}
\subsection{Overview}
To specify the coordination at language level, approaches rely on partial knowledge on the syntax and behavioral semantic of the languages they coordinate, \ie a language behavioral interface. At the model level, the notion of interface is used to expose information of model behavior in order to ease its coordination (see Section~\ref{ch:background}). Similarly, at language level, a behavioral interface is used to expose only a part of the syntax and semantic of language to ease its coordination. It abstracts the behavioral semantics of a language, thus providing only the information required to coordinate it, \ie a partial representation of concurrency and time-related aspects. In this sense, a language behavioral interface is specific for coordination purpose.    

In the state of art approaches, the notion of language behavioral varies depending on the approach. For example in Ptolemy~\cite{ptoleframebib} and ModHel'X~\cite{modhelxbib}, the behavior of models is described by a director that implements a domain in Java, \eg FSM, DE, SDF. To support the communication between different domains, each director has a Java interface made of methods that provides a homogeneous view of the behavioral semantics of the domain. Each method has a semantics that is expressed in natural language. For instance, a part of the \emph{fire()} function description is: ``\emph{Typically, the fire() method performs the computation associated with an actor}''.
 	
%\footnote{the Executable interface, cf. http://chess.eecs.berkeley.edu/ptexternal/src/ptII/doc/codeDoc/ptolemy/actor/Executable.html}
While in frameworks the notion of interface is clearly defined, in Di Natale et al.~\cite{dinatale} information about the syntax and semantics of the functional and platform languages is contained into the mapping language. First, it contains correspondences between syntactic elements from both languages, \eg \emph{Task}s from the functional and \emph{Cpu}s from the platform. Second, to express the semantics of such correspondences, the translation semantics of the mapping language is based on knowledge about the translation semantics of the coordinated languages. For instance, it contains the name of the methods that are generated for each subsystem in the functional language. Such methods are part of the translational semantics of the functional language. The mapping language encodes the knowledge about What elements from the languages are coordinated. In this approach, such a notion of interface is implicit into the mapping language. 

In Mascot~\cite{mascotbib}, Matlab models are embedded into SDL models by relying on SDL wrappers. The SDL wrapper has access to a C-wrapper implemented as a set of C functions, which calls the Matlab engine through a set of C-library functions. Some of these functions are \emph{InitProcess()} that initializes a data-flow process in Matlab and \emph{OutStream()} that writes a data-flow variable in Matlab. By relying on these methods, the SDL engine drives the simulation and calls the Matlab engine to process dataflow signals. The coordination results not symmetric since the communication is only done from SDL to Matlab. In this sense, there is non an interface for SDL models. Only matlab is interfaced by relying on C wrappers that act as an interface made of methods.
\subsection{Discussion}
All approaches get information about the languages that they coordinate. However, only Ptolemy~\cite{ptoleframebib} and Modhel'X~\cite{modhelxbib} made such information explicit by relying on a language interface. Conversely, Di Natale et al.~\cite{dinatale} and MASCOT~\cite{mascotbib} encode such information either into a dedicated language or in wrappers. When such information is hard-coded, the task of adding or changing a language becomes difficult. For instance, in~\cite{dinatale}, the mapping language hides the information about what elements are coordinated. Thus, to add or change a language, a developer has to first identify what elements from the languages are used as coordination points. Conversely, in Ptolemy~\cite{ptoleframebib} and ModHel'X~\cite{modhelxbib} such coordination points are explicitly identified into a generic interface. Thus, to add a new language, it is only necessary to provide the correct interface. However, in these approaches, the generic interface only provides a list of methods in which the semantics is given by theirs names. The methods can be interpreted differently depending on the domain, thus remaining very informal. Besides, a system designer has to understand well the framework to know how to use them. 

At the model level,~\cite{garlansoftarchbib} explains that there are important benefits of using events (with implicit invocation) in the component interfaces because it provides strong support for reuse and evolution of components. In event-driven coordination approaches, events act as explicit coordination points and exhibit what can be coordinated. This gives support for control and timed coordination while remaining independent of the internal model implementation, thus ensuring a complete separation between the coordination and the computational concerns. Several causal representations from the concurrency theory are used to capture event-based behavioral interface. A causal representation captures the concurrency, dependency and conflict relationships among actions in a particular program. For instance, an event structure~\cite{eventStructures} is a partial order of events, which specifies the, possibly timed, causality relations as well as conflict relations (\ie exclusion relations) between actions of a concurrent system. This fundamental model is powerful because it totally abstracts data and program structure to focus on the partial ordering of actions. It specifies, \emph{in extension} and \emph{in order}, the set of actions that can be observed during the program execution. An event structure can also be specified \emph{in intention} to represent the set of observable event structures during an execution (see \eg\cite{ccslbib} or \cite{tagmachinebib}).

In~\cite{sle13-combemale} elements of event structures are reified at the language level to propose a behavioral interface based on sets of \emph{event types} and \emph{contraints}. Event types (named \dse for Domain Specific Event) are defined in the context of a metaclass of the abstract syntax (\as), and abstract the relevant semantic actions. Jointly with the \dse, related constraints give a symbolic (intentional) representation of an event structure. With such an interface, the concurrency and time-related aspects of the language behavioral semantics are explicitly exposed. Furthermore, languages can be coordinated without changing current implementations. 

%\todo{In this approach, for each model conforming to a language, the model behavioral interface is a specification, in intention, of an event structure whose events (named \mse for Model Specific Event) are instances of the \dse defined in the language interface. While \dse are attached to a metaclass, \mse are linked to one of its instances. The causality and conflict relations of the event structure are a model-specific unfolding of the constraints specified in the language behavioral interface. Just like event structures were initially introduced to unfold the execution of Petri nets, we use them here to unfold the execution of models.}	    	

	    	
	    	
	    				%\item In~\cite{taminghetero}, authors rely on the interface automata to specify the 
	    				%\item An interesting research direction could be to adapt the notion of interface automata proposed in~\cite{henzingerIA} to specify the acceptable protocol between the calls of the functions that implements the semantics of the language. 
	    				
	    				
	    				%However, because the language interface definition used in \cite{ptolemybib} and \cite{modhelxbib} provides only a list of functions, it requires a deep understanding of the underlying framework to be able to specify correct coordination patterns. A naming convention on the name of the methods remains very informal since the same methods may be have a different meaning depending on the domain.        
	    				
	    				%\item All the approaches studied retrieve information on the syntax and/or behavioral semantics of the languages they coordinate. Only \cite{ptolemybib} and \cite{modhelxbib} made explicit the notion of interface; \cite{MarcoModels2014} and \cite{mascotbib} retrieved the information implicitly from the definition of the languages. 
	    				%Using an explicit language interface is a way to obtain a language independent representation of the behavioral semantics. In this case, the approaches can add support to new languages with a minimum effort since they are seen homogeneously. However, because the language interface definition used in \cite{ptolemybib} and \cite{modhelxbib} provides only a list of functions, it requires a deep understanding of the underlying framework to be able to specify correct coordination patterns. 
	    				
	    				%\item This task can be helped by .... 
	    				%\item \todo{An interesting research direction could be to adapt the notion of interface automata proposed in~\cite{henzingerIA} to specify the acceptable protocol between the calls of the functions that implements the semantics of the language.} 
	    				